# -*- coding: utf-8 -*-
"""Final weather and cases.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_t6MhwQuEc0Iia6LEXbdM1_cH0Rwmq1V
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, TimeSeriesSplit, cross_val_score
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from statsmodels.tsa.arima.model import ARIMA

df = pd.read_excel("/content/citywise_average_by year - CLEANED.xlsx")

# Drop irrelevant columns
df.drop(columns=['City Name'], inplace=True)

# EDA
print(df.info())
print(df.describe())
print(df.isnull().sum())  # Check missing values

# Correlation Heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt='.2f')
plt.title("Feature Correlation Heatmap")
plt.show()

# Handling Outliers (Using IQR Method)
Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
df = df[~((df < lower_bound) | (df > upper_bound)).any(axis=1)]

# Feature Engineering - Polynomial Features
poly = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly.fit_transform(df.drop(columns=['Year', 'No of Patients ']))

# Feature Scaling
scaler = StandardScaler()
df_scaled = pd.DataFrame(scaler.fit_transform(df.drop(columns=['Year', 'No of Patients '])), columns=df.columns[2:])
df_scaled['Year'] = df['Year'].values
df_scaled['No of Patients '] = df['No of Patients '].values

# Splitting Data for Linear Regression
X = df_scaled.drop(columns=['No of Patients '])
y = df_scaled['No of Patients ']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Ridge and Lasso Regression
ridge = Ridge(alpha=1.0)
ridge.fit(X_train, y_train)
lasso = Lasso(alpha=0.1)
lasso.fit(X_train, y_train)

# Predictions
y_pred_ridge = ridge.predict(X_test)
y_pred_lasso = lasso.predict(X_test)

# Regression Evaluation
print("Ridge Regression Performance:")
print(f"MAE: {mean_absolute_error(y_test, y_pred_ridge):.4f}")
print(f"MSE: {mean_squared_error(y_test, y_pred_ridge):.4f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_ridge)):.4f}")
print(f"R2 Score: {r2_score(y_test, y_pred_ridge):.4f}")

print("Lasso Regression Performance:")
print(f"MAE: {mean_absolute_error(y_test, y_pred_lasso):.4f}")
print(f"MSE: {mean_squared_error(y_test, y_pred_lasso):.4f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_lasso)):.4f}")
print(f"R2 Score: {r2_score(y_test, y_pred_lasso):.4f}")

# Regression Evaluation
print("Ridge Regression Performance:")
print(f"MAE: {mean_absolute_error(y_test, y_pred_ridge):.4f}")
print(f"MSE: {mean_squared_error(y_test, y_pred_ridge):.4f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_ridge)):.4f}")
print(f"R2 Score: {r2_score(y_test, y_pred_ridge):.4f}")
print(f"Ridge Regression Accuracy: {r2_score(y_test, y_pred_ridge) * 100:.2f}%")

print("Lasso Regression Performance:")
print(f"MAE: {mean_absolute_error(y_test, y_pred_lasso):.4f}")
print(f"MSE: {mean_squared_error(y_test, y_pred_lasso):.4f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_lasso)):.4f}")
print(f"R2 Score: {r2_score(y_test, y_pred_lasso):.4f}")
print(f"Lasso Regression Accuracy: {r2_score(y_test, y_pred_lasso) * 100:.2f}%")

# ARIMA Model for Time Series Forecasting
df_arima = df[['Year', 'No of Patients ']].groupby('Year').sum()
model_arima = ARIMA(df_arima, order=(5,1,0))
model_arima_fit = model_arima.fit()

# Time Series Cross Validation
# Reduced the number of splits to a maximum of 3,
# to be less than the number of samples (4).
# This ensures there are enough samples for each fold.
ts_split = TimeSeriesSplit(n_splits=2)  # Reduced n_splits to 2

for train_index, test_index in ts_split.split(df_arima):
    train, test = df_arima.iloc[train_index], df_arima.iloc[test_index]

    # Check if the training data has enough samples for ARIMA(5,1,1)
    if len(train) > 5:  # At least 6 samples are needed for (5,1,1)
        model = ARIMA(train, order=(5,1,1))
        model_fit = model.fit()
        predictions = model_fit.forecast(steps=len(test))
        print(f"Fold MSE: {mean_squared_error(test, predictions):.4f}")
    else:
        print("Not enough data points in the training set for this fold.")

# Train Final ARIMA Model
model_arima = ARIMA(df_arima, order=(5,1,1))
model_arima_fit = model_arima.fit()

# Predict Future Values
forecast_years = [2021, 2022, 2023]
predictions_arima = model_arima_fit.forecast(steps=len(forecast_years))

# Plot Predictions
plt.figure(figsize=(10,5))
plt.plot(df_arima.index, df_arima['No of Patients '], label="Actual Patients", marker='o')
plt.plot(forecast_years, predictions_arima, label="Predicted Patients", marker='s', linestyle='dashed')
plt.xlabel("Year")
plt.ylabel("No of Patients")
plt.title("Patient Forecasting using ARIMA")
plt.legend()
plt.show()

# Future Predictions using Regression
future_years = [2021, 2022, 2023, 2024, 2025]
future_weather_data = np.random.rand(len(future_years), scaler.n_features_in_)
future_scaled = scaler.transform(future_weather_data)

# ARIMA Model Evaluation (Using MAPE for accuracy)
actual = df_arima['No of Patients '].values
predicted = model_arima_fit.fittedvalues.values
mape = np.mean(np.abs((actual - predicted) / actual)) * 100
print(f"ARIMA Model Accuracy: {100 - mape:.2f}%")