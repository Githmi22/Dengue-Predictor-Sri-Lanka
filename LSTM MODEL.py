# -*- coding: utf-8 -*-
"""assignment 1 chamo

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ja1YeJ0Qj3f0s9cK4inyHFHfiBTTzR40
"""

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import TimeSeriesSplit, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import mean_absolute_error, mean_squared_error

# Load dataset
file_path = "CLEANED.xlsx"  # Update the path if needed
df = pd.read_excel(file_path, sheet_name="Year-wise Averages")

# Select relevant columns (now including 'City Name')
df = df[['City Name', 'Year', 'No of Patients ', 'temperature_2m_mean (Â°C)',
         'precipitation_sum (mm)', 'rain_sum (mm)', 'wind_speed_10m_max (km/h)', 'sunshine_duration (s)']]

# Rename columns for easier handling
df.columns = ['City', 'Year', 'Patients', 'Temperature', 'Precipitation', 'Rainfall', 'WindSpeed', 'Sunshine']

# Encode 'City' as a numerical feature
label_encoder = LabelEncoder()
df['City_Code'] = label_encoder.fit_transform(df['City'])

# Create lag features for each city
df = df.sort_values(by=['City', 'Year'])  # Ensure data is sorted properly

for lag in range(1, 4):
    df[f'Patients_Lag{lag}'] = df.groupby('City')['Patients'].shift(lag)

# Rolling average of past 3 years for each city
df['Patients_RollingAvg'] = df.groupby('City')['Patients'].transform(lambda x: x.rolling(window=3).mean())

# Drop rows with NaN values due to shifting
df.dropna(inplace=True)

# Define features (X) and target variable (y)
features = ['City_Code', 'Patients_Lag1', 'Patients_Lag2', 'Patients_Lag3', 'Patients_RollingAvg',
            'Temperature', 'Precipitation', 'Rainfall', 'WindSpeed', 'Sunshine']

X = df[features]
y = df['Patients']

# Standardize features (except City_Code, which is categorical)
scaler = StandardScaler()
X_scaled = X.copy()
X_scaled.iloc[:, 1:] = scaler.fit_transform(X.iloc[:, 1:])

# TimeSeriesSplit for better validation
tscv = TimeSeriesSplit(n_splits=5)

# Train XGBoost model
xgb_model = xgb.XGBRegressor(n_estimators=200, learning_rate=0.05, max_depth=5, random_state=42)
xgb_model.fit(X_scaled, y)

# Cross-validation
cv_scores = cross_val_score(xgb_model, X_scaled, y, cv=tscv, scoring='neg_mean_absolute_error')
mae_cv = -np.mean(cv_scores)

# Predictions
y_pred = xgb_model.predict(X_scaled)

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.ensemble import RandomForestRegressor

# Load dataset (replace with actual dataset)
X, y = np.random.rand(100, 5), np.random.rand(100)  # Example data

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Hyperparameter tuning using GridSearchCV
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10],
}

model = RandomForestRegressor(random_state=42)
grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_root_mean_squared_error')
grid_search.fit(X_train_scaled, y_train)

# Best model
best_model = grid_search.best_estimator_

# Predictions
y_pred = best_model.predict(X_test_scaled)

# Evaluation
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

# Cross-validation MAE
mae_cv = -cross_val_score(best_model, X_train_scaled, y_train, cv=5, scoring='neg_mean_absolute_error').mean()

# Display results
print(f"Optimized Model - Best Params: {grid_search.best_params_}")
print(f"Mean Absolute Error (MAE): {mae}")
print(f"Root Mean Squared Error (RMSE): {rmse}")
print(f"Cross-Validated MAE: {mae_cv}")

# Model Evaluation
mae = mean_absolute_error(y, y_pred)
rmse = np.sqrt(mean_squared_error(y, y_pred))

print(f"Mean Absolute Error (MAE): {mae}")
print(f"Root Mean Squared Error (RMSE): {rmse}")

print(f"Cross-Validated MAE: {mae_cv}")
print(f"Mean Absolute Error (MAE): {mae}")
print(f"Root Mean Squared Error (RMSE): {rmse}")

# Predict dengue cases for 2025 for each city
future_predictions = {}

for city in df['City'].unique():
    latest_data = df[df['City'] == city].iloc[-1][features]

    # Shift lag features forward for 2025
    future_data = latest_data.copy()
    future_data['Patients_Lag3'] = future_data['Patients_Lag2']
    future_data['Patients_Lag2'] = future_data['Patients_Lag1']
    future_data['Patients_Lag1'] = np.nan  # Placeholder for 2025

     # Update rolling average (assuming a simple estimate)
    future_data['Patients_RollingAvg'] = (future_data['Patients_Lag1'] + future_data['Patients_Lag2'] + future_data['Patients_Lag3']) / 3

    # Standardize input
    future_data_scaled = scaler.transform([future_data[1:]])  # Exclude City_Code from scaling

# Dictionary to store predictions
future_predictions = {}

# List of numeric features to be scaled
numeric_features = ['Patients_Lag1', 'Patients_Lag2', 'Patients_Lag3', 'Patients_RollingAvg',
                    'Temperature', 'Precipitation', 'Rainfall', 'WindSpeed', 'Sunshine']

for city in df['City'].unique():
    # Get the latest available data for the city
    latest_data = df[df['City'] == city].iloc[-1].copy()

    # Create future input data
    future_data = latest_data.copy()

    # Shift lag features forward for 2025
    future_data['Patients_Lag3'] = future_data['Patients_Lag2']
    future_data['Patients_Lag2'] = future_data['Patients_Lag1']
    future_data['Patients_Lag1'] = future_data['Patients']

    # Update rolling average (estimate based on new lag values)
    future_data['Patients_RollingAvg'] = (future_data['Patients_Lag1'] +
                                          future_data['Patients_Lag2'] +
                                          future_data['Patients_Lag3']) / 3

    # Ensure it's a DataFrame
    future_data_df = pd.DataFrame([future_data])

    # Standardize numeric features (City_Code is not scaled)
    future_data_df[numeric_features] = scaler.transform(future_data_df[numeric_features])

    # Select the correct feature columns
    future_data_df = future_data_df[features]

    # Predict for 2025
    predicted_2025 = xgb_model.predict(future_data_df)[0]
    future_predictions[city] = predicted_2025

# Print predictions for each city
print("\nPredicted Dengue Cases for 2025 by City:")
for city, prediction in future_predictions.items():
    print(f"{city}: {round(prediction)} cases")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.arima.model import ARIMA
from sklearn.metrics import mean_absolute_error, mean_squared_error

# Example: Load time series data
data = pd.read_excel('/content/CLEANED.xlsx', sheet_name='Year-wise Averages')

# Split data into train and test sets
train_size = int(len(data) * 0.8)
train, test = data[:train_size], data[train_size:]

# Fit ARIMA Model (Change (p,d,q) values as needed)
model = ARIMA(train['No of Patients '], order=(1, 1, 1))
model_fit = model.fit()

# Make predictions
predictions = model_fit.forecast(steps=len(test))

# Calculate Accuracy Metrics
mse = mean_squared_error(test['No of Patients '], predictions)
mae = mean_absolute_error(test['No of Patients '], predictions)
rmse = np.sqrt(mse)
mape = np.mean(np.abs((test - predictions) / test)) * 100  # Mean Absolute Percentage Error

# Print results
print(f"MAE: {mae:.4f}")
print(f"MSE: {mse:.4f}")
print(f"RMSE: {rmse:.4f}")
print(f"MAPE: {mape:.2f}%")

# Plot actual vs predicted values
plt.figure(figsize=(10,5))
plt.plot(test.index, test['No of Patients '], label="Actual", color='blue')
plt.plot(test.index, predictions, label="Predicted", color='red', linestyle="dashed")
plt.legend()
plt.title("ARIMA Model - Actual vs Predicted")
plt.show()